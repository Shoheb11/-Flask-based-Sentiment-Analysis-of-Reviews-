{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset from a TSV (Tab-Separated Values) file.\n",
    "# The dataset contains restaurant reviews.\n",
    "df_train = pd.read_csv('../../datasets/Restaurant_Reviews.tsv', sep='\\t')\n",
    "\n",
    "# Displaying the first 5 rows of the dataset to get an initial look at the data.\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the list of English stopwords from the NLTK library.\n",
    "# Stopwords are commonly used words (like \"and\", \"the\", \"is\") that are often removed during text preprocessing\n",
    "# because they do not carry significant meaning for tasks like sentiment analysis or text classification.\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the total number of English stopwords in the NLTK stopwords list.\n",
    "# This will return the count of words considered as stopwords in the English language.\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the word 'not' from the stopwords list.\n",
    "# 'Not' is often important in sentiment analysis as it can negate the sentiment (e.g., \"not good\").\n",
    "stopwords.remove('not')\n",
    "# Removing the word 'but' from the stopwords list.\n",
    "# 'But' can also carry significant meaning as it indicates contrast or emphasis (e.g., \"good but expensive\").\n",
    "stopwords.remove('but')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the Porter Stemmer from the NLTK library.\n",
    "# The Porter Stemmer is used to reduce words to their base or root form (e.g., \"loved\" -> \"love\").\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stemmer.stem('loved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(r1):\n",
    "    # Converting the input text (r1) to lowercase to ensure uniformity.\n",
    "    r1 = r1.lower()\n",
    "    # Removing all characters except lowercase alphabets using a regular expression.\n",
    "    # This replaces non-alphabetic characters with a space.\n",
    "    r1 = re.sub('[^a-z]', ' ', r1)\n",
    "    # Tokenizing the text into words, removing stopwords, and applying stemming.\n",
    "    # 'r1.split()' splits the text into words.\n",
    "    # The condition 'if word not in stopwords' filters out stopwords.\n",
    "    # 'stemmer.stem(word)' reduces each word to its stem (base form).\n",
    "    words = [stemmer.stem(word) for word in r1.split() if word not in stopwords]\n",
    "    # Joining the processed words back into a single string with spaces between them.\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow love place'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess('Wow... Loved this place.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the preprocess function to each review in the 'Review' column of the dataframe.\n",
    "# The processed reviews are stored in a new Series called 'preprocessed_reviews'.\n",
    "preprocessed_reviews = df_train['Review'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the CountVectorizer to create a Bag-of-Words (BoW) model.\n",
    "# The 'ngram_range=(1,2)' parameter specifies that both unigrams (single words) and bigrams (two consecutive words) \n",
    "# will be considered as features in the vectorization process.\n",
    "# Alternatively, you could use TfidfVectorizer() to create a TF-IDF model for feature extraction.\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))  # TfidfVectorizer()\n",
    "\n",
    "# Fitting the vectorizer to the preprocessed reviews and transforming them into a sparse matrix.\n",
    "# Each row in the matrix corresponds to a review, and each column corresponds to a unique word or bigram.\n",
    "bow_table = vectorizer.fit_transform(preprocessed_reviews)\n",
    "\n",
    "# Converting the sparse matrix to a dense array.\n",
    "# This allows for compatibility with machine learning algorithms that may require dense input.\n",
    "X_train = bow_table.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5755)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the shape of the 'X_train' array, which represents the number of rows (reviews) and columns (features/terms).\n",
    "# The shape will be in the form (number_of_reviews, number_of_features).\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the 'Liked' column from the dataframe to the variable 'y_train'.\n",
    "# This column represents the target labels (e.g., whether the review is positive or negative),\n",
    "# which will be used for supervised learning during model training.\n",
    "y_train = df_train['Liked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the LogisticRegression class from the scikit-learn library.\n",
    "# Logistic Regression is a classification algorithm used for binary or multiclass classification tasks.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initializing the Logistic Regression model.\n",
    "rmodel = LogisticRegression()\n",
    "\n",
    "# Fitting the model to the training data.\n",
    "# The model will learn from the input features in 'X_train' and the corresponding target labels in 'y_train'.\n",
    "# This step trains the model to predict the 'Liked' column based on the Bag-of-Words features of the reviews.\n",
    "rmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating the performance of the trained model on the training data.\n",
    "# The 'score' method calculates the accuracy of the model by comparing the predicted labels to the true labels in 'y_train'.\n",
    "# It returns the proportion of correctly classified instances in the training dataset.\n",
    "rmodel.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9979838709677419\n",
      "Recall: 0.99\n",
      "F1 Score: 0.9939759036144579\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary metrics from scikit-learn to evaluate the model's performance.\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Using the trained model to predict the target labels for the training data.\n",
    "y_pred = rmodel.predict(X_train)\n",
    "\n",
    "# Calculating the Precision, Recall, and F1 Score for the model's predictions.\n",
    "# Precision: The proportion of true positive predictions among all positive predictions.\n",
    "precision = precision_score(y_train, y_pred)\n",
    "\n",
    "# Recall: The proportion of true positive predictions among all actual positive instances.\n",
    "recall = recall_score(y_train, y_pred)\n",
    "\n",
    "# F1 Score: The harmonic mean of Precision and Recall, providing a balanced measure of the model's performance.\n",
    "f1 = f1_score(y_train, y_pred)\n",
    "\n",
    "# Printing the calculated metrics\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a list of sample reviews that will be used for testing the model.\n",
    "# These reviews contain both positive and negative feedback about food and service.\n",
    "x = [\n",
    "    'Food was tasty and service was fast...',\n",
    "    'The food was very delicious! Loved your service too',\n",
    "    'Very good test.... I love it....',\n",
    "    'Waiting time is very high',\n",
    "    'Unhappy with your service…',\n",
    "    'Wonderful food! Absolutely amazing.',\n",
    "    'Authentic dishes, cozy ambiance and excellent service make it must visit',\n",
    "    'pathetic service but Nice food.',\n",
    "    'The food and service quality was excellent'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a pandas Series from the list of test reviews 'x'.\n",
    "# This Series will allow us to apply the preprocessing function to each review individually.\n",
    "s1 = pd.Series(x)\n",
    "\n",
    "# Applying the 'preprocess' function to each review in the Series to clean and normalize the text.\n",
    "# This step ensures that the text is in a consistent format before feature extraction.\n",
    "s1 = s1.apply(preprocess)\n",
    "\n",
    "# Transforming the preprocessed test reviews into feature vectors using the same vectorizer used for training data.\n",
    "# The 'transform' method is used instead of 'fit_transform' since we are applying it to new, unseen data.\n",
    "# The result is a sparse matrix of features (terms from n-grams).\n",
    "X_test = vectorizer.transform(s1).toarray()\n",
    "\n",
    "# Using the trained model to predict the sentiment (liked/disliked) for the test reviews.\n",
    "# This returns the predicted labels for each review in the test set.\n",
    "rmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rmodel.pkl', 'wb') as file:\n",
    "    pickle.dump(rmodel, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
